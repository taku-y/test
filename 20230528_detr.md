---
marp: true
theme: default
# header: "**ヘッダータイトル1** __ヘッダータイトル2__"
# footer: "by **＠NISHIZONO**"

size: 16:9
paginate: true

style: |
    section.title {
        --title-height: 130px;
        --subtitle-height: 70px;

        overflow: visible;
        display: grid;
        grid-template-columns: 1fr;
        grid-template-rows: 1fr var(--title-height) var(--subtitle-height) 1fr;
        grid-template-areas: "." "title" "subtitle" ".";
    }

    section.title h1,
    section.title h2 {
        margin: 0;
        padding: 0;
        text-align: center;
        height: var(--area-height);
        line-height: var(--area-height);
        font-size: calc(var(--area-height) * 0.7);

        border: 1px dashed gray; /* debug */
    }

    section.title h1 {
        grid-area: title;
        --area-height: var(--title-height);
    }

    section.title h2 {
        grid-area: subtitle;
        --area-height: var(--subtitle-height);
    }

    section {
        justify-content: start;
    }

---
<!-- _class: title -->
# DETRの調査
<!-- ## サブタイトル -->

---
## DETRの位置付け
### Transformerを用いた画像処理の先駆け
- DEtector Transformer (DETR; [Carion et al., 2020](https://arxiv.org/abs/2005.12872))
    - CNN特徴抽出 + Transformer
- Swin transformer ([Liu et al., 2021](https://arxiv.org/abs/2103.14030))
    - Shifted windowを用いたtransformer
![w:300](https://cdn-ak.f.st-hatena.com/images/fotolife/k/kurumatu/20220817/20220817221912.png)
- DETRwithImproveddeNoisinganchOr boxes (DINO; [Zhang et al., 2022](https://arxiv.org/abs/2203.03605))
    - DETRの改良

---

### より高度な画像処理の基礎
- Grounding DINO ([Liu et al., 2023](https://arxiv.org/abs/2303.05499))
    - プロンプトによるオープンセットの物体検出器
    - クローズセット（検出対象が固定）の検出器であるDINOの拡張
![w:400](https://blog.shikoan.com/wp-content/uploads/2023/04/grounding_dino_03.png)
- Segment Any Anomaly+ (SAA+; [Cao et al., 2023](https://arxiv.org/pdf/2305.10724.pdf))
    - Zero-shot anomaly segmentation (ZSAS)のプロンプトによる正則化
    - Grounding DINOを使用

---
## DETRのアーキテクチャと処理の概要
![h:192](https://www.ogis-ri.co.jp/otc/hiroba/technical/detr/img/pic202109-005.png)
https://www.ogis-ri.co.jp/otc/hiroba/technical/detr/part1.html

- CNNによる特徴抽出とpositional encodingによる低解像度活動マップの計算（1, 2）
- 活動マップのエンコーディング（3）
- オブジェクトクエリのデコーディング（4）
- デコードされたオブジェクトクエリから物体クラスとBBを予測（5）

---
## 低解像度活動マップ
![h:204](https://www.ogis-ri.co.jp/otc/hiroba/technical/detr/img/pic202109-006.png)
- 元画像（$H\times W$）をCNNに適用し低解像度の活動マップ（$H'\times W'$）を取得
    - 活動マップのサイズは論文によると$H'=H/32, W'=W/32$
- 活動マップの各要素をトークンとして扱う
    - 元の活動マップ$H'\times W'\times d$をトークン列$(H'W')\times d$に変換, $d$は特徴量次元
- 補足：CNN出力のチャネル数$C$は線形変換で$d$に圧縮される
    - 論文では$C=2048$, $d=256$

---
## エンコーディング
- 言語モデルで用いられるような普通のエンコーダを用いているらしい
- 最近の画像Transformerモデルはアテンションの範囲を空間的に限定している
    - Swin transformer
    - Deformable attention transformer ([Xia et al., 2022](https://arxiv.org/pdf/2201.00520.pdf))
![bg right h:600](https://deepsquare.jp/wp-content/uploads/2022/01/Screenshot-from-2022-01-12-15-09-38.jpg)

---
## デコーディング
- 「オブジェクトクエリ」と呼ばれるベクトルをデコーディングする
    - デコーディングされたベクトルからクラスとBBを計算
    - オブジェクトクエリの数 = 検出可能な最大物体数（ハイパーパラメータ）
    - オブジェクトクエリはモデルパラメータの一部
    - ランダムに初期化され, 訓練後（推論時）は固定
        - 訓練データ中に現れる対象の空間分布を学習しているように見える
            - https://www.youtube.com/watch?v=utxbUlo9CyY&t=327s
            - COCOデータセットに対する各オブジェクトクエリの平均位置
    - （系列生成とは異なり）オブジェクトクエリは並列に処理される

---
## 損失関数
- モデルが出力する（クラス, BB）の組の集合と, 訓練データの画像に現れる（クラス, BB）の組の集合を二部マッチングして損失関数とする
![](https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F1412761%2F10292ba5-9109-36f7-ad1d-9641b68e3b78.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=32f08873c624bf313719f3ec0aede4bf)
- DETRではハンガリアンアルゴリズムを使用
- 集合同士の距離を計算（参考：[最適輸送の解き方](https://www.slideshare.net/joisino/ss-249394573)）
